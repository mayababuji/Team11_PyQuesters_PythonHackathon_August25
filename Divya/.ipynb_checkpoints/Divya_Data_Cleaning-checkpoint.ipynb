{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2149704a-c464-4121-9189-ad29cb8de2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three datasets are created \n",
    "# These Datasets are created after cleaning all the datasets provided \n",
    "# All_PATIENTS.csv This Dataset contains all 25 patients data from HCPA000*P.csv\n",
    "# DEMOGRAPHIC.csv This dataset contains demogrpahic data provided in  \"T1DM_patient_sleep_demographics_with_race.csv\"\n",
    "# PATINETS_WITH_DEMOGRAPHIC.csv This dataset is merge of above two datasets\n",
    "# This Jupyter notebooks has cells:\n",
    "    # importing modules\n",
    "    # declaring variables\n",
    "    # creating a dataframe by merging all csv files having patient data\n",
    "    # change time into date and time and write the clean data into csv file\n",
    "    # dataframe for demographic data by renaming all column names to lowercase with underscore betweeen words and created csv file\n",
    "    # dataframe for merged allpatients and demographic data and created csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58b4f00b-1e7b-42e7-a5e8-70602a5cf529",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the necessary libraries\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaa2f031-b490-44ab-8fd8-229b17f4238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#global variables\n",
    "folder_path=\"patients_data\"\n",
    "file_pattern=\"HUPA*.csv\",\n",
    "delimiter=\";\"\n",
    "#Variables\n",
    "#path = \"../HUPA-UC Diabetes Dataset/\"\n",
    "demographic_path = folder_path + \"T1DM_patient_sleep_demographics_with_race.csv\"\n",
    "all_patients_path = path + \"ALL_PATIENTS.csv\"\n",
    "modified_demographic_path = path + \"DEMOGRAPHIC.csv\"\n",
    "patients_demographic_path = path +\"PATIENTS_WITH_DEMOGRAPHIC.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab62eb04-083f-4f18-9518-183625063bfc",
   "metadata": {},
   "source": [
    "combined all 25 patient files. cleaned and saved to the final_patients.csv file\n",
    "Removes any repeated rows.\n",
    "Why: Duplicate entries can skew statistics, especially in time-series or patient-level analysis.\n",
    "Drops rows where either time or glucose is missing.\n",
    "Why: These are critical fields for tracking blood sugar trends. Without them, the data point is unusable.\n",
    "Converts the time column to the proper datetime format.\n",
    "Why: Ensures consistency for sorting, filtering, and time-based feature engineering.\n",
    "Note: errors='coerce' turns invalid formats into NaT (missing), which is handled in the next step\n",
    "Removes rows where time couldn't be parsed.\n",
    "Why: Keeps only rows with valid timestamps for chronological analysis.\n",
    "Sorts the data by patient and time, then resets the index.\n",
    "Prepares the dataset for time-series modeling, rolling averages, or lag feature creation per patient.\n",
    "No duplicate or incomplete records\n",
    "Valid timestamps for temporal analysis\n",
    "Chronological ordering per patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf678ecf-ad6c-4c02-902a-ed17accf69e3",
   "metadata": {},
   "outputs": [],
   "source": [
    " Step 1: Load Existing Final File (if it exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "10b9ba31-8767-40cf-822c-3cabddb69714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ No existing file found. Starting fresh.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load existing final file\n",
    "final_path = \"final_patients.csv\"\n",
    "if os.path.exists(final_path):\n",
    "    existing_df = pd.read_csv(final_path)\n",
    "    print(f\"‚úÖ Loaded existing file with {len(existing_df)} records\")\n",
    "else:\n",
    "    existing_df = pd.DataFrame()\n",
    "    print(\"üìÇ No existing file found. Starting fresh.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f333b4-7165-4c19-a366-8032aeca9158",
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 2: Load New Patient Files and combine all the 25 files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "23755f25-e59d-44fc-b6c0-8ab9a67093a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Found 25 patient files\n",
      "üÜï Loaded new batch with 309392 records\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# Load new files\n",
    "folder_path = \"patients_data\"\n",
    "file_pattern = \"HUPA*.csv\"\n",
    "file_list = glob.glob(os.path.join(folder_path, file_pattern))\n",
    "print(f\"üìÅ Found {len(file_list)} patient files\")\n",
    "\n",
    "# Load and tag each file\n",
    "df_list = []\n",
    "for file in file_list:\n",
    "    temp_df = pd.read_csv(file, delimiter=\";\")\n",
    "    temp_df['patient_id'] = os.path.basename(file).split(\".\")[0]\n",
    "    df_list.append(temp_df)\n",
    "\n",
    "# Combine all new files\n",
    "new_df = pd.concat(df_list, ignore_index=True)\n",
    "print(f\"üÜï Loaded new batch with {len(new_df)} records\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7310e50d-e189-4d89-b50c-929f07069580",
   "metadata": {},
   "source": [
    " Step 3: Clean New Data\n",
    "Removes any repeated rows.\n",
    "Why: Duplicate entries can skew statistics, especially in time-series or patient-level analysis.\n",
    "Drops rows where either time or glucose is missing.\n",
    "Why: These are critical fields for tracking blood sugar trends. Without them, the data point is unusable.\n",
    "Converts the time column to the proper datetime format.\n",
    "Why: Ensures consistency for sorting, filtering, and time-based feature engineering.\n",
    "Note: errors='coerce' turns invalid formats into NaT (missing), which is handled in the next step\n",
    "Removes rows where time couldn't be parsed.\n",
    "Why: Keeps only rows with valid timestamps for chronological analysis.\n",
    "Sorts the data by patient and time, then resets the index.\n",
    "Prepares the dataset for time-series modeling, including rolling averages and lag feature creation, per patient.\n",
    "No duplicate or incomplete records\n",
    "Valid timestamps for temporal analysis\n",
    "Chronological ordering per patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c95458b2-bb8a-4327-9ed2-cb8a17ec8263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßº Cleaned new data: 309392 records remain\n"
     ]
    }
   ],
   "source": [
    "new_df = new_df.drop_duplicates()\n",
    "new_df = new_df.dropna(subset=['time', 'glucose'])\n",
    "new_df['time'] = pd.to_datetime(new_df['time'], errors='coerce')\n",
    "new_df = new_df.dropna(subset=['time'])\n",
    "new_df = new_df.sort_values(by=['patient_id', 'time']).reset_index(drop=True)\n",
    "print(f\"üßº Cleaned new data: {len(new_df)} records remain\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d282e9ce-bb07-4886-9eb9-8ed9afd5f6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 4: Merge and Deduplicate\n",
    "Here‚Äôs the critical part ‚Äî we‚Äôll deduplicate based on key columns only:\n",
    "This ensures that even if the same file is reprocessed, it won‚Äôt duplicate rows unless glucose or time differs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "21155457-3268-46e4-a6a8-281daffb630f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Combined total: 309392 records after deduplication\n"
     ]
    }
   ],
   "source": [
    "# Merge and deduplicate\n",
    "combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "combined_df = combined_df.drop_duplicates(subset=['patient_id', 'time', 'glucose']).reset_index(drop=True)\n",
    "print(f\"üîó Combined total: {len(combined_df)} records after deduplication\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d21738-1abe-4bba-a1f3-84949b7ca858",
   "metadata": {},
   "outputs": [],
   "source": [
    "Splitting time into date and time helps correlate patient events with demographic factors like \n",
    "age, gender, or region by enabling daily or hourly trend analysis. \n",
    "It also supports clinical insights‚Äîsuch as identifying post-meal glucose spikes, sleep-related vitals, \n",
    "or medication timing effects‚Äîby aligning temporal patterns with medical context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9a0d1f03-d4b1-4690-9dcc-83baa4671111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Final reshaped data saved to 'final_patients.csv'\n"
     ]
    }
   ],
   "source": [
    "# Split 'time' into 'date' and 'time_only'\n",
    "combined_df['date'] = combined_df['time'].dt.date\n",
    "combined_df['time_only'] = combined_df['time'].dt.time\n",
    "\n",
    "# Reorder columns: patient_id, date, time, then the rest\n",
    "ordered_cols = ['patient_id', 'date', 'time_only'] + [\n",
    "    col for col in combined_df.columns \n",
    "    if col not in ['patient_id', 'date', 'time_only', 'time']\n",
    "]\n",
    "combined_df = combined_df[ordered_cols]\n",
    "\n",
    "# Rename 'time_only' back to 'time' for clarity\n",
    "combined_df.rename(columns={'time_only': 'time'}, inplace=True)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Final reshaped data saved to 'final_patients.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7690af-feda-4859-a745-dfdd87d4b19c",
   "metadata": {},
   "outputs": [],
   "source": [
    " Step 5: Save Final File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "aece9cb9-1d99-4d09-882d-00405d57af82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Final saved to final_patients.csv\n"
     ]
    }
   ],
   "source": [
    "combined_df.to_csv(final_path, index=False)\n",
    "print(f\"‚úÖ Final saved to {final_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9944d4bd-d589-4b91-8935-11b3a77547aa",
   "metadata": {},
   "outputs": [],
   "source": [
    " Step 6: Preview the Final CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a0814cf1-be46-49c7-8013-1c63258bebd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Final file has 309392 records and 10 columns\n",
      "patient_id       date     time  glucose  calories  heart_rate  steps  basal_rate  bolus_volume_delivered  carb_input\n",
      " HUPA0001P 2018-06-13 18:40:00    332.0    6.3595   82.322835   34.0    0.091667                     0.0         0.0\n",
      " HUPA0001P 2018-06-13 18:45:00    326.0    7.7280   83.740157    0.0    0.091667                     0.0         0.0\n",
      " HUPA0001P 2018-06-13 18:50:00    330.0    4.7495   80.525180    0.0    0.091667                     0.0         0.0\n",
      " HUPA0001P 2018-06-13 18:55:00    324.0    6.3595   89.129032   20.0    0.091667                     0.0         0.0\n",
      " HUPA0001P 2018-06-13 19:00:00    306.0    5.1520   92.495652    0.0    0.075000                     0.0         0.0\n"
     ]
    }
   ],
   "source": [
    "# Load and preview the saved final file\n",
    "final_df = pd.read_csv(final_path)\n",
    "\n",
    "# Basic info\n",
    "print(f\"üìä Final file has {len(final_df)} records and {final_df.shape[1]} columns\")\n",
    "\n",
    "# Show first few rows\n",
    "print(final_df.head().to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1c8bead6-e6d4-4a05-8dac-e3e989f68be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def combine_clean_save_patients(\n",
    "    folder_path=\"patients_data\",\n",
    "    file_pattern=\"HUPA*.csv\",\n",
    "    output_file=\"final_patients.csv\",\n",
    "    processed_log=\"processed_files.txt\",\n",
    "    delimiter=\";\",\n",
    "    verbose=True\n",
    "):\n",
    "    # Step 1: Load existing cleaned file\n",
    "    existing_df = pd.read_csv(output_file) if os.path.exists(output_file) else pd.DataFrame()\n",
    "    if verbose:\n",
    "        print(f\"üìÇ {'Existing file found' if not existing_df.empty else 'No existing file found. Starting fresh.'}\")\n",
    "\n",
    "    # Step 2: Load processed file log\n",
    "    processed_files = set()\n",
    "    if os.path.exists(processed_log):\n",
    "        with open(processed_log, \"r\") as f:\n",
    "            processed_files = set(f.read().splitlines())\n",
    "\n",
    "    # Step 3: Load new files\n",
    "    file_list = glob.glob(os.path.join(folder_path, file_pattern))\n",
    "    new_files = [f for f in file_list if os.path.basename(f) not in processed_files]\n",
    "    if verbose:\n",
    "        print(f\"üìÅ Found {len(new_files)} new patient files\")\n",
    "\n",
    "    df_list = []\n",
    "    for file in new_files:\n",
    "        try:\n",
    "            temp_df = pd.read_csv(file, delimiter=delimiter)\n",
    "            patient_id = os.path.basename(file).split(\".\")[0]\n",
    "            temp_df['patient_id'] = patient_id\n",
    "            df_list.append(temp_df)\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"‚ö†Ô∏è Skipping file {file} due to error: {e}\")\n",
    "\n",
    "    if not df_list:\n",
    "        if verbose:\n",
    "            print(\"‚ö†Ô∏è No new data to process.\")\n",
    "        return existing_df\n",
    "\n",
    "    # Step 4: Combine and clean\n",
    "    new_df = pd.concat(df_list, ignore_index=True)\n",
    "    new_df = new_df.drop_duplicates()\n",
    "    new_df = new_df.dropna(subset=['time', 'glucose'])\n",
    "    new_df['time'] = pd.to_datetime(new_df['time'], errors='coerce')\n",
    "    new_df = new_df.dropna(subset=['time'])\n",
    "    new_df = new_df.sort_values(by=['patient_id', 'time']).reset_index(drop=True)\n",
    "\n",
    "    # Step 5: Merge and deduplicate by key columns\n",
    "    combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "    combined_df = combined_df.drop_duplicates(subset=['patient_id', 'time', 'glucose']).reset_index(drop=True)\n",
    "\n",
    "    # Step 6: Save updated data and log\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "    with open(processed_log, \"a\") as f:\n",
    "        for file in new_files:\n",
    "            f.write(os.path.basename(file) + \"\\n\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"‚úÖ Final cleaned data saved to: {output_file}\")\n",
    "        print(f\"üßÆ Total records: {len(combined_df)}\")\n",
    "        print(f\"üìä Columns: {combined_df.columns.tolist()}\")\n",
    "        print(\"üîç Sample rows:\")\n",
    "        print(combined_df.head().to_string(index=False))\n",
    "\n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e2ba7b29-f7ba-4e9a-b5e6-e3e82cb39a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Existing file found: final_patients.csv\n",
      "üìÅ Found 25 patient files\n",
      "‚úÖ Final cleaned data saved to: final_patients.csv\n",
      "üßÆ Total records: 618872\n",
      "üìä Columns: ['patient_id', 'time', 'glucose', 'calories', 'heart_rate', 'steps', 'basal_rate', 'bolus_volume_delivered', 'carb_input']\n",
      "üîç Sample rows:\n",
      "patient_id                time  glucose  calories  heart_rate  steps  basal_rate  bolus_volume_delivered  carb_input\n",
      " HUPA0001P 2018-06-13 18:40:00    332.0    6.3595   82.322835   34.0    0.091667                     0.0         0.0\n",
      " HUPA0001P 2018-06-13 18:45:00    326.0    7.7280   83.740157    0.0    0.091667                     0.0         0.0\n",
      " HUPA0001P 2018-06-13 18:50:00    330.0    4.7495   80.525180    0.0    0.091667                     0.0         0.0\n",
      " HUPA0001P 2018-06-13 18:55:00    324.0    6.3595   89.129032   20.0    0.091667                     0.0         0.0\n",
      " HUPA0001P 2018-06-13 19:00:00    306.0    5.1520   92.495652    0.0    0.075000                     0.0         0.0\n"
     ]
    }
   ],
   "source": [
    "df = combine_clean_save_patients()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50829700-a976-438e-aa01-a257a5977e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned the final_patients file with the time column seperated to date,clock, weekday, hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad21113a-14bc-46bd-8a51-a8483bd46643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Enriched Data Summary\n",
      "Total records: 618784\n",
      "Columns: ['patient_id', 'time', 'glucose', 'calories', 'heart_rate', 'steps', 'basal_rate', 'bolus_volume_delivered', 'carb_input', 'date', 'clock_time', 'hour', 'dayofweek']\n",
      "üîç Sample rows:\n",
      "patient_id                time  glucose  calories  heart_rate  steps  basal_rate  bolus_volume_delivered  carb_input       date clock_time  hour dayofweek\n",
      " HUPA0001P 2018-06-13 18:40:00    332.0    6.3595   82.322835   34.0    0.091667                     0.0         0.0 2018-06-13   18:40:00    18 Wednesday\n",
      " HUPA0001P 2018-06-13 18:45:00    326.0    7.7280   83.740157    0.0    0.091667                     0.0         0.0 2018-06-13   18:45:00    18 Wednesday\n",
      " HUPA0001P 2018-06-13 18:50:00    330.0    4.7495   80.525180    0.0    0.091667                     0.0         0.0 2018-06-13   18:50:00    18 Wednesday\n",
      " HUPA0001P 2018-06-13 18:55:00    324.0    6.3595   89.129032   20.0    0.091667                     0.0         0.0 2018-06-13   18:55:00    18 Wednesday\n",
      " HUPA0001P 2018-06-13 19:00:00    306.0    5.1520   92.495652    0.0    0.075000                     0.0         0.0 2018-06-13   19:00:00    19 Wednesday\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load cleaned data\n",
    "df_enriched = pd.read_csv(\"final_patients.csv\")\n",
    "\n",
    "# Step 2: Add enriched columns\n",
    "df_enriched['date'] = pd.to_datetime(df['time']).dt.date\n",
    "df_enriched['clock_time'] = pd.to_datetime(df['time']).dt.time\n",
    "df_enriched['hour'] = pd.to_datetime(df['time']).dt.hour\n",
    "df_enriched['dayofweek'] = pd.to_datetime(df['time']).dt.day_name()\n",
    "\n",
    "# Step 3: Save enriched version\n",
    "df_enriched.to_csv(\"final_patients_enriched.csv\", index=False)\n",
    "# Step 4: Preview enriched data\n",
    "print(\"üìä Enriched Data Summary\")\n",
    "print(f\"Total records: {len(df_enriched)}\")\n",
    "print(f\"Columns: {df_enriched.columns.tolist()}\")\n",
    "print(\"üîç Sample rows:\")\n",
    "print(df_enriched.head().to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537e4f3e-0ce8-4bd7-8a9d-7c39c90903a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename the T1DM_patient_sleep_demographics_with_race file column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0b0ad0de-d908-4e8b-b200-501a57af4cbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'demographic_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[101], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Renamed Column names as per the HUPA standred naming conventions and created new csv file\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m demographic_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(demographic_path)\n\u001b[0;32m      3\u001b[0m demographic_df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPatient_ID\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpatient_id\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAge\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGender\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgender\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRace\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrace\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Sleep Duration (hrs)\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_sleep_duration_hrs\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSleep Quality (1-10)\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msleep_quality_score\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m with Sleep Disturbances\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msleep_disturbances_percentage\u001b[39m\u001b[38;5;124m\"\u001b[39m},inplace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m demographic_df\u001b[38;5;241m.\u001b[39mcolumns\n",
      "\u001b[1;31mNameError\u001b[0m: name 'demographic_path' is not defined"
     ]
    }
   ],
   "source": [
    "#Renamed Column names as per the HUPA standred naming conventions and created new csv file\n",
    "demographic_df = pd.read_csv(demographic_path)\n",
    "demographic_df.rename(columns={\"Patient_ID\":\"patient_id\",\"Age\":\"age\",\"Gender\":\"gender\",\"Race\":\"race\",\"Average Sleep Duration (hrs)\":\"avg_sleep_duration_hrs\",\"Sleep Quality (1-10)\":\"sleep_quality_score\",\"% with Sleep Disturbances\":\"sleep_disturbances_percentage\"},inplace = True)\n",
    "demographic_df.columns\n",
    "#saving the data to the cleaned_demographics.csv file \n",
    "demographic_df.to_csv(\"cleaned_demographics.csv\", index=False)\n",
    "print(df_demo.head().to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396851ab-d5aa-4405-889c-502ac9c2ea08",
   "metadata": {},
   "outputs": [],
   "source": [
    "combining cleaned_demographies and enriched patient data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39664562-662a-4cfe-8b1c-1d1a9fef2d56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Merged Data Summary\n",
      "Total records: 309392\n",
      "Columns: ['patient_id', 'time', 'glucose', 'calories', 'heart_rate', 'steps', 'basal_rate', 'bolus_volume_delivered', 'carb_input', 'date', 'clock_time', 'hour', 'dayofweek', 'age', 'gender', 'race', 'avg_sleep_hours', 'sleep_quality', 'sleep_disturbances_pct']\n",
      "üß™ Sample rows:\n",
      "patient_id                time  glucose  calories  heart_rate  steps  basal_rate  bolus_volume_delivered  carb_input       date clock_time  hour dayofweek  age gender  race  avg_sleep_hours  sleep_quality  sleep_disturbances_pct\n",
      " HUPA0001P 2018-06-13 18:40:00    332.0    6.3595   82.322835   34.0    0.091667                     0.0         0.0 2018-06-13   18:40:00    18 Wednesday   34   Male Other              6.3            4.5                      80\n",
      " HUPA0001P 2018-06-13 18:45:00    326.0    7.7280   83.740157    0.0    0.091667                     0.0         0.0 2018-06-13   18:45:00    18 Wednesday   34   Male Other              6.3            4.5                      80\n",
      " HUPA0001P 2018-06-13 18:50:00    330.0    4.7495   80.525180    0.0    0.091667                     0.0         0.0 2018-06-13   18:50:00    18 Wednesday   34   Male Other              6.3            4.5                      80\n",
      " HUPA0001P 2018-06-13 18:55:00    324.0    6.3595   89.129032   20.0    0.091667                     0.0         0.0 2018-06-13   18:55:00    18 Wednesday   34   Male Other              6.3            4.5                      80\n",
      " HUPA0001P 2018-06-13 19:00:00    306.0    5.1520   92.495652    0.0    0.075000                     0.0         0.0 2018-06-13   19:00:00    19 Wednesday   34   Male Other              6.3            4.5                      80\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Step 1: Load enriched patient data\n",
    "df_patients = pd.read_csv(\"final_patients_enriched.csv\")\n",
    "\n",
    "# Step 2: Load demographics data\n",
    "df_demograph = pd.read_csv(\"cleaned_demographics.csv\")\n",
    "\n",
    "# Step 3: Merge on patient_id\n",
    "df_merged = pd.merge(df_patients, df_demograph, on=\"patient_id\", how=\"left\")\n",
    "\n",
    "# Step 4: Preview merged data\n",
    "print(\"üîó Merged Data Summary\")\n",
    "print(f\"Total records: {len(df_merged)}\")\n",
    "print(f\"Columns: {df_merged.columns.tolist()}\")\n",
    "print(\"üß™ Sample rows:\")\n",
    "print(df_merged.head().to_string(index=False))\n",
    "\n",
    "# Step 5: Save merged version\n",
    "df_merged.to_csv(\"final_patients_full.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5114a07-5a64-4aab-b08a-77174bdeea10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned and reordered. Saved to 'final_patients_cleaned.csv'\n",
      "üß¨ New column order: ['patient_id', 'date', 'time', 'dayofweek', 'glucose', 'calories', 'heart_rate', 'steps', 'basal_rate', 'bolus_volume_delivered', 'carb_input', 'hour', 'age', 'gender', 'race', 'avg_sleep_hours', 'sleep_quality', 'sleep_disturbances_pct']\n",
      "üîç Sample rows:\n",
      "patient_id       date     time dayofweek  glucose  calories  heart_rate  steps  basal_rate  bolus_volume_delivered  carb_input  hour  age gender  race  avg_sleep_hours  sleep_quality  sleep_disturbances_pct\n",
      " HUPA0001P 2018-06-13 18:40:00 Wednesday    332.0    6.3595   82.322835   34.0    0.091667                     0.0         0.0    18   34   Male Other              6.3            4.5                      80\n",
      " HUPA0001P 2018-06-13 18:45:00 Wednesday    326.0    7.7280   83.740157    0.0    0.091667                     0.0         0.0    18   34   Male Other              6.3            4.5                      80\n",
      " HUPA0001P 2018-06-13 18:50:00 Wednesday    330.0    4.7495   80.525180    0.0    0.091667                     0.0         0.0    18   34   Male Other              6.3            4.5                      80\n",
      " HUPA0001P 2018-06-13 18:55:00 Wednesday    324.0    6.3595   89.129032   20.0    0.091667                     0.0         0.0    18   34   Male Other              6.3            4.5                      80\n",
      " HUPA0001P 2018-06-13 19:00:00 Wednesday    306.0    5.1520   92.495652    0.0    0.075000                     0.0         0.0    19   34   Male Other              6.3            4.5                      80\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"final_patients_full.csv\")  # Adjust path if needed\n",
    "\n",
    "# Step 1: Drop the original 'time' column\n",
    "df.drop(columns=['time'], inplace=True)\n",
    "\n",
    "# Step 2: Rename 'clock_time' to 'time'\n",
    "df.rename(columns={'clock_time': 'time'}, inplace=True)\n",
    "\n",
    "# Step 3: Reorder columns\n",
    "front_cols = ['patient_id', 'date', 'time', 'dayofweek']\n",
    "remaining_cols = [col for col in df.columns if col not in front_cols]\n",
    "new_order = front_cols + remaining_cols\n",
    "df = df[new_order]\n",
    "\n",
    "# Step 4: Save the updated file\n",
    "df.to_csv(\"final_patients_cleaned.csv\", index=False)\n",
    "\n",
    "# Step 5: Preview\n",
    "print(\"‚úÖ Cleaned and reordered. Saved to 'final_patients_cleaned.csv'\")\n",
    "print(\"üß¨ New column order:\", df.columns.tolist())\n",
    "print(\"üîç Sample rows:\")\n",
    "print(df.head().to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e879854b-5933-4d8c-841c-59551e6f228f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning: 309392\n",
      "['patient_id', 'date', 'time', 'dayofweek', 'glucose', 'calories', 'heart_rate', 'steps', 'basal_rate', 'bolus_volume_delivered', 'carb_input', 'hour', 'age', 'gender', 'race', 'avg_sleep_hours', 'sleep_quality', 'sleep_disturbances_pct']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"final_patients_cleaned.csv\")\n",
    "print(f\"Before cleaning: {len(df)}\")\n",
    "print(df.columns.tolist())\n",
    "df.drop_duplicates(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9057e52a-bde3-45e2-94d3-1a5c8e6e0542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 309281\n",
      "Invalid records dropped: 111\n"
     ]
    }
   ],
   "source": [
    "# Define rules\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"final_patients_cleaned.csv\")\n",
    "\n",
    "\n",
    "rules = {\n",
    "    'glucose': lambda x: 40 <= x <= 400,\n",
    "    'heart_rate': lambda x: 30 <= x <= 220,\n",
    "    'steps': lambda x: 0 <= x <= 50000,\n",
    "    'basal_rate': lambda x: 0.0 <= x <= 3.0,\n",
    "    'bolus_volume_delivered': lambda x: 0.0 <= x <= 25.0,\n",
    "    'carb_input': lambda x: 0 <= x <= 300,\n",
    "    'avg_sleep_hours': lambda x: 0 <= x <= 24,\n",
    "    'sleep_quality': lambda x: 0 <= x <= 100,\n",
    "    'sleep_disturbances_pct': lambda x: 0 <= x <= 100,\n",
    "    'age': lambda x: 0 <= x <= 120,\n",
    "    'hour': lambda x: 0 <= x <= 23\n",
    "}\n",
    "\n",
    "# Flag invalid rows\n",
    "invalid_rows = pd.DataFrame()\n",
    "for col, rule in rules.items():\n",
    "    if col in df.columns:\n",
    "        mask = ~df[col].apply(lambda x: rule(x) if pd.notnull(x) else True)\n",
    "        invalid_rows = pd.concat([invalid_rows, df[mask]])\n",
    "\n",
    "# Drop invalid rows\n",
    "df_cleaned = df.drop(invalid_rows.index)\n",
    "df_cleaned.to_csv(\"final_patients_cleaned_validated.csv\", index=False)\n",
    "invalid_rows.drop_duplicates().to_csv(\"invalid_entries_log.csv\", index=False)\n",
    "print(f\"Total records: {len(df_cleaned)}\")\n",
    "print(f\"Invalid records dropped: {len(invalid_rows.drop_duplicates())}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "69aadb69-42a4-4503-958b-ad8cea25bcc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 4096\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"patients_data/HUPA0001P.csv\")\n",
    "print(f\"Total records: {len(df1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf78676-e548-498f-863e-0b5bfafea8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dataframe by merging all csv files having patient data\n",
    "csv_files = glob.glob(os.path.join(path,\"H*.csv\"))\n",
    "df_list =[]\n",
    "for file in csv_files:\n",
    "    patientId = os.path.basename(file).split(\".\")[0]\n",
    "    df = pd.read_csv(file,sep=\";\")\n",
    "    df[\"patient_id\"] = patientId\n",
    "    df_list.append(df)\n",
    "patients_df = pd.concat(df_list, ignore_index = True)\n",
    "patients_df.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
